{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "dca0ade3e726a953b501b15e8e990130d2b7799f14cfd9f4271676035ebe5511"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all necessary packages\n",
    "%matplotlib inline\n",
    "# Load all necessary packages\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.algorithms.preprocessing import Reweighing\n",
    "from aif360.datasets import StandardDataset\n",
    "from IPython.display import Markdown, display\n",
    "from aif360.metrics import ClassificationMetric\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from common_utils import compute_metrics\n",
    "from aif360.algorithms.preprocessing.optim_preproc_helpers.data_preproc_functions\\\n",
    "        import load_preproc_data_adult, load_preproc_data_german, load_preproc_data_compas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Load dataset, specifying protected attribute, and split dataset into train and testÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../data/compas_florida.csv')\n",
    "dataset_orig = StandardDataset(df,label_name='recidivism_within_2_years', favorable_classes=[1], protected_attribute_names=['race'], privileged_classes = [['Caucasian']], features_to_keep=['sex_num','age','priors_count', 'juv_fel_count', 'juv_misd_count', 'juv_other_count'])\n",
    "# privileged_groups = [{'race': 1}]\n",
    "# unprivileged_groups = [{'race': 0}]\n",
    "# dataset_orig = load_preproc_data_compas(['race'])    # attribute for \"sex\" which we do not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_orig_train, dataset_orig_test = dataset_orig.split([0.7], shuffle=True)\n",
    "privileged_groups = [{'race': 1}]\n",
    "unprivileged_groups = [{'race': 0}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dataset and split into train and test\n",
    "dataset_orig_train, dataset_orig_vt = dataset_orig.split([0.7], shuffle=True)\n",
    "dataset_orig_valid, dataset_orig_test = dataset_orig_vt.split([0.5], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "#### Training Dataset shape"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(12235, 7)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "#### Favorable and unfavorable labels"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.0 0.0\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "#### Protected attribute names"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['race']\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "#### Privileged and unprivileged protected attribute values"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[array([1.])] [array([0.])]\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "#### Dataset feature names"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['age', 'race', 'juv_fel_count', 'juv_misd_count', 'juv_other_count', 'priors_count', 'sex_num']\n"
     ]
    }
   ],
   "source": [
    "# print out some labels, names, etc.\n",
    "display(Markdown(\"#### Training Dataset shape\"))\n",
    "print(dataset_orig_train.features.shape)\n",
    "display(Markdown(\"#### Favorable and unfavorable labels\"))\n",
    "print(dataset_orig_train.favorable_label, dataset_orig_train.unfavorable_label)\n",
    "display(Markdown(\"#### Protected attribute names\"))\n",
    "print(dataset_orig_train.protected_attribute_names)\n",
    "display(Markdown(\"#### Privileged and unprivileged protected attribute values\"))\n",
    "print(dataset_orig_train.privileged_protected_attributes, \n",
    "      dataset_orig_train.unprivileged_protected_attributes)\n",
    "display(Markdown(\"#### Dataset feature names\"))\n",
    "print(dataset_orig_train.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "#### Original training dataset"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Difference in mean outcomes between unprivileged and privileged groups = 0.089035\n"
     ]
    }
   ],
   "source": [
    "## Step 3 Compute fairness metric on original training dataset\n",
    "metric_orig_train = BinaryLabelDatasetMetric(dataset_orig_train, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "display(Markdown(\"#### Original training dataset\"))\n",
    "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_orig_train.mean_difference())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mitigate bias by transforming the original dataset\n",
    "RW = Reweighing(unprivileged_groups=unprivileged_groups,\n",
    "                privileged_groups=privileged_groups)\n",
    "dataset_transf_train = RW.fit_transform(dataset_orig_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "#### Transformed training dataset"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Difference in mean outcomes between unprivileged and privileged groups = 0.000000\n"
     ]
    }
   ],
   "source": [
    "#Compute fairness metric on transformed dataset\n",
    "metric_transf_train = BinaryLabelDatasetMetric(dataset_transf_train, \n",
    "                                               unprivileged_groups=unprivileged_groups,\n",
    "                                               privileged_groups=privileged_groups)\n",
    "display(Markdown(\"#### Transformed training dataset\"))\n",
    "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_transf_train.mean_difference())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train classifier on original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression classifier and predictions\n",
    "scale_orig = StandardScaler()\n",
    "X_train = scale_orig.fit_transform(dataset_orig_train.features)\n",
    "y_train = dataset_orig_train.labels.ravel()\n",
    "w_train = dataset_orig_train.instance_weights.ravel()\n",
    "\n",
    "lmod = LogisticRegression()\n",
    "lmod.fit(X_train, y_train, \n",
    "         sample_weight=dataset_orig_train.instance_weights)\n",
    "y_train_pred = lmod.predict(X_train)\n",
    "\n",
    "# positive class index\n",
    "pos_ind = np.where(lmod.classes_ == dataset_orig_train.favorable_label)[0][0]\n",
    "\n",
    "dataset_orig_train_pred = dataset_orig_train.copy()\n",
    "dataset_orig_train_pred.labels = y_train_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Obtain scores for original validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_orig_valid_pred = dataset_orig_valid.copy(deepcopy=True)\n",
    "X_valid = scale_orig.transform(dataset_orig_valid_pred.features)\n",
    "y_valid = dataset_orig_valid_pred.labels\n",
    "dataset_orig_valid_pred.scores = lmod.predict_proba(X_valid)[:,pos_ind].reshape(-1,1)\n",
    "\n",
    "dataset_orig_test_pred = dataset_orig_test.copy(deepcopy=True)\n",
    "X_test = scale_orig.transform(dataset_orig_test_pred.features)\n",
    "y_test = dataset_orig_test_pred.labels\n",
    "dataset_orig_test_pred.scores = lmod.predict_proba(X_test)[:,pos_ind].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best balanced accuracy (no reweighing) = 0.6551\nOptimal classification threshold (no reweighing) = 0.4852\n"
     ]
    }
   ],
   "source": [
    "#Find the optimal classification threshold from the validation set\n",
    "num_thresh = 100\n",
    "ba_arr = np.zeros(num_thresh)\n",
    "class_thresh_arr = np.linspace(0.01, 0.99, num_thresh)\n",
    "for idx, class_thresh in enumerate(class_thresh_arr):\n",
    "    \n",
    "    fav_inds = dataset_orig_valid_pred.scores > class_thresh\n",
    "    dataset_orig_valid_pred.labels[fav_inds] = dataset_orig_valid_pred.favorable_label\n",
    "    dataset_orig_valid_pred.labels[~fav_inds] = dataset_orig_valid_pred.unfavorable_label\n",
    "    \n",
    "    classified_metric_orig_valid = ClassificationMetric(dataset_orig_valid,\n",
    "                                             dataset_orig_valid_pred, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "    \n",
    "    ba_arr[idx] = 0.5*(classified_metric_orig_valid.true_positive_rate()\\\n",
    "                       +classified_metric_orig_valid.true_negative_rate())\n",
    "\n",
    "best_ind = np.where(ba_arr == np.max(ba_arr))[0][0]\n",
    "best_class_thresh = class_thresh_arr[best_ind]\n",
    "\n",
    "print(\"Best balanced accuracy (no reweighing) = %.4f\" % np.max(ba_arr))\n",
    "print(\"Optimal classification threshold (no reweighing) = %.4f\" % best_class_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "#### Predictions from original testing data"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|ââââââââââ| 100/100 [00:00<00:00, 22212.06it/s]Classification threshold used = 0.4852\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Predictions from the original test set at the optimal classification threshold\n",
    "display(Markdown(\"#### Predictions from original testing data\"))\n",
    "bal_acc_arr_orig = []\n",
    "disp_imp_arr_orig = []\n",
    "avg_odds_diff_arr_orig = []\n",
    "\n",
    "print(\"Classification threshold used = %.4f\" % best_class_thresh)\n",
    "for thresh in tqdm(class_thresh_arr):\n",
    "    \n",
    "    if thresh == best_class_thresh:\n",
    "        disp = True\n",
    "    else:\n",
    "        disp = False\n",
    "    \n",
    "    fav_inds = dataset_orig_test_pred.scores > thresh\n",
    "    dataset_orig_test_pred.labels[fav_inds] = dataset_orig_test_pred.favorable_label\n",
    "    dataset_orig_test_pred.labels[~fav_inds] = dataset_orig_test_pred.unfavorable_label\n",
    "    \n",
    "    # metric_test_bef = compute_metrics(dataset_orig_test, dataset_orig_test_pred, \n",
    "    #                                   unprivileged_groups, privileged_groups,\n",
    "    #                                   disp = disp)\n",
    "\n",
    "    # bal_acc_arr_orig.append(metric_test_bef[\"Balanced accuracy\"])\n",
    "    # avg_odds_diff_arr_orig.append(metric_test_bef[\"Average odds difference\"])\n",
    "    # disp_imp_arr_orig.append(metric_test_bef[\"Disparate impact\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train classifier on transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_transf = StandardScaler()\n",
    "X_train = scale_transf.fit_transform(dataset_transf_train.features)\n",
    "y_train = dataset_transf_train.labels.ravel()\n",
    "\n",
    "lmod = LogisticRegression()\n",
    "lmod.fit(X_train, y_train,\n",
    "        sample_weight=dataset_transf_train.instance_weights)\n",
    "y_train_pred = lmod.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0., 0., 1., ..., 0., 1., 1.])"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "y_train_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtain scores for transformed test set\n",
    "dataset_transf_test_pred = dataset_orig_test.copy(deepcopy=True)\n",
    "X_test = scale_transf.fit_transform(dataset_transf_test_pred.features)\n",
    "y_test = dataset_transf_test_pred.labels\n",
    "dataset_transf_test_pred.scores = lmod.predict_proba(X_test)[:,pos_ind].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_test_pred = lmod.predict(X_test)\n",
    "model_accuracy = accuracy_score(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.6414950419527079"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "model_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####post processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.algorithms.postprocessing.calibrated_eq_odds_postprocessing import CalibratedEqOddsPostprocessing\n",
    "from tqdm import tqdm\n",
    "cost_constraint = \"fnr\" \n",
    "randseed = 12345679 \n",
    "cpp = CalibratedEqOddsPostprocessing(privileged_groups = privileged_groups,\n",
    "                                     unprivileged_groups = unprivileged_groups,\n",
    "                                     cost_constraint=cost_constraint,\n",
    "                                     seed=randseed)\n",
    "cpp = cpp.fit(dataset_orig_valid, dataset_orig_valid_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_transf_valid_pred = cpp.predict(dataset_orig_valid_pred)\n",
    "dataset_transf_test_pred = cpp.predict(dataset_orig_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "#### Original-Transformed validation dataset"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Difference in GFPR between unprivileged and privileged groups\n-0.00929442668326641\nDifference in GFNR between unprivileged and privileged groups\n-0.0002603628813890313\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "#### Original-Transformed testing dataset"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Difference in GFPR between unprivileged and privileged groups\n0.005358614712775767\nDifference in GFNR between unprivileged and privileged groups\n-0.02049611672565471\n"
     ]
    }
   ],
   "source": [
    "cm_transf_valid = ClassificationMetric(dataset_orig_valid, dataset_transf_valid_pred,\n",
    "                             unprivileged_groups=unprivileged_groups,\n",
    "                             privileged_groups=privileged_groups)\n",
    "display(Markdown(\"#### Original-Transformed validation dataset\"))\n",
    "print(\"Difference in GFPR between unprivileged and privileged groups\")\n",
    "print(cm_transf_valid.difference(cm_transf_valid.generalized_false_positive_rate))\n",
    "print(\"Difference in GFNR between unprivileged and privileged groups\")\n",
    "print(cm_transf_valid.difference(cm_transf_valid.generalized_false_negative_rate))\n",
    "\n",
    "cm_transf_test = ClassificationMetric(dataset_orig_test, dataset_transf_test_pred,\n",
    "                             unprivileged_groups=unprivileged_groups,\n",
    "                             privileged_groups=privileged_groups)\n",
    "display(Markdown(\"#### Original-Transformed testing dataset\"))\n",
    "print(\"Difference in GFPR between unprivileged and privileged groups\")\n",
    "print(cm_transf_test.difference(cm_transf_test.generalized_false_positive_rate))\n",
    "print(\"Difference in GFNR between unprivileged and privileged groups\")\n",
    "print(cm_transf_test.difference(cm_transf_test.generalized_false_negative_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 2.649326; batch adversarial loss: 0.693051\n",
      "epoch 1; iter: 0; batch classifier loss: 0.719812; batch adversarial loss: 0.683652\n",
      "epoch 2; iter: 0; batch classifier loss: 0.783496; batch adversarial loss: 0.666361\n",
      "epoch 3; iter: 0; batch classifier loss: 0.676851; batch adversarial loss: 0.638366\n",
      "epoch 4; iter: 0; batch classifier loss: 0.585254; batch adversarial loss: 0.637448\n",
      "epoch 5; iter: 0; batch classifier loss: 0.769580; batch adversarial loss: 0.644483\n",
      "epoch 6; iter: 0; batch classifier loss: 0.670961; batch adversarial loss: 0.621023\n",
      "epoch 7; iter: 0; batch classifier loss: 0.687964; batch adversarial loss: 0.656516\n",
      "epoch 8; iter: 0; batch classifier loss: 0.623074; batch adversarial loss: 0.655481\n",
      "epoch 9; iter: 0; batch classifier loss: 0.624079; batch adversarial loss: 0.645491\n",
      "epoch 10; iter: 0; batch classifier loss: 0.628514; batch adversarial loss: 0.618160\n",
      "epoch 11; iter: 0; batch classifier loss: 0.609667; batch adversarial loss: 0.559828\n",
      "epoch 12; iter: 0; batch classifier loss: 0.617629; batch adversarial loss: 0.646708\n",
      "epoch 13; iter: 0; batch classifier loss: 0.623369; batch adversarial loss: 0.672917\n",
      "epoch 14; iter: 0; batch classifier loss: 0.698612; batch adversarial loss: 0.567221\n",
      "epoch 15; iter: 0; batch classifier loss: 0.627505; batch adversarial loss: 0.624291\n",
      "epoch 16; iter: 0; batch classifier loss: 0.673244; batch adversarial loss: 0.675920\n",
      "epoch 17; iter: 0; batch classifier loss: 0.625226; batch adversarial loss: 0.643568\n",
      "epoch 18; iter: 0; batch classifier loss: 0.658992; batch adversarial loss: 0.625324\n",
      "epoch 19; iter: 0; batch classifier loss: 0.565143; batch adversarial loss: 0.616011\n",
      "epoch 20; iter: 0; batch classifier loss: 0.601155; batch adversarial loss: 0.631101\n",
      "epoch 21; iter: 0; batch classifier loss: 0.642321; batch adversarial loss: 0.607252\n",
      "epoch 22; iter: 0; batch classifier loss: 0.624429; batch adversarial loss: 0.598087\n",
      "epoch 23; iter: 0; batch classifier loss: 0.638708; batch adversarial loss: 0.656323\n",
      "epoch 24; iter: 0; batch classifier loss: 0.624121; batch adversarial loss: 0.610106\n",
      "epoch 25; iter: 0; batch classifier loss: 0.594156; batch adversarial loss: 0.615328\n",
      "epoch 26; iter: 0; batch classifier loss: 0.638836; batch adversarial loss: 0.669813\n",
      "epoch 27; iter: 0; batch classifier loss: 0.614864; batch adversarial loss: 0.588317\n",
      "epoch 28; iter: 0; batch classifier loss: 0.706570; batch adversarial loss: 0.592441\n",
      "epoch 29; iter: 0; batch classifier loss: 0.630596; batch adversarial loss: 0.593262\n",
      "epoch 30; iter: 0; batch classifier loss: 0.600628; batch adversarial loss: 0.632906\n",
      "epoch 31; iter: 0; batch classifier loss: 0.623438; batch adversarial loss: 0.667824\n",
      "epoch 32; iter: 0; batch classifier loss: 0.634506; batch adversarial loss: 0.659618\n",
      "epoch 33; iter: 0; batch classifier loss: 0.658465; batch adversarial loss: 0.665148\n",
      "epoch 34; iter: 0; batch classifier loss: 0.653719; batch adversarial loss: 0.585031\n",
      "epoch 35; iter: 0; batch classifier loss: 0.637875; batch adversarial loss: 0.645024\n",
      "epoch 36; iter: 0; batch classifier loss: 0.606602; batch adversarial loss: 0.595349\n",
      "epoch 37; iter: 0; batch classifier loss: 0.679628; batch adversarial loss: 0.654708\n",
      "epoch 38; iter: 0; batch classifier loss: 0.582912; batch adversarial loss: 0.628656\n",
      "epoch 39; iter: 0; batch classifier loss: 0.593942; batch adversarial loss: 0.548945\n",
      "epoch 40; iter: 0; batch classifier loss: 0.634913; batch adversarial loss: 0.623305\n",
      "epoch 41; iter: 0; batch classifier loss: 0.593300; batch adversarial loss: 0.658712\n",
      "epoch 42; iter: 0; batch classifier loss: 0.596113; batch adversarial loss: 0.676392\n",
      "epoch 43; iter: 0; batch classifier loss: 0.644517; batch adversarial loss: 0.597279\n",
      "epoch 44; iter: 0; batch classifier loss: 0.633580; batch adversarial loss: 0.677916\n",
      "epoch 45; iter: 0; batch classifier loss: 0.619414; batch adversarial loss: 0.631316\n",
      "epoch 46; iter: 0; batch classifier loss: 0.553358; batch adversarial loss: 0.638862\n",
      "epoch 47; iter: 0; batch classifier loss: 0.630551; batch adversarial loss: 0.581270\n",
      "epoch 48; iter: 0; batch classifier loss: 0.587521; batch adversarial loss: 0.644160\n",
      "epoch 49; iter: 0; batch classifier loss: 0.694107; batch adversarial loss: 0.679760\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<aif360.algorithms.inprocessing.adversarial_debiasing.AdversarialDebiasing at 0x1a6912f590>"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "## in processing ---- adversarial debaising\n",
    "from aif360.algorithms.inprocessing.adversarial_debiasing import AdversarialDebiasing\n",
    "import tensorflow.compat.v1 as tf\n",
    "sess.close()\n",
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "debiased_model = AdversarialDebiasing(privileged_groups = privileged_groups,\n",
    "                          unprivileged_groups = unprivileged_groups,\n",
    "                          scope_name='debiased_classifier',\n",
    "                          debias=True,\n",
    "                          sess=sess)\n",
    "debiased_model.fit(dataset_orig_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_debiasing_train = debiased_model.predict(dataset_orig_train)\n",
    "dataset_debiasing_test = debiased_model.predict(dataset_orig_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "#### Model - with debiasing - dataset metrics"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train set: Difference in mean outcomes between unprivileged and privileged groups = 0.077185\nTest set: Difference in mean outcomes between unprivileged and privileged groups = 0.085240\n"
     ]
    }
   ],
   "source": [
    "# # Metrics for the dataset from plain model (without debiasing)\n",
    "# display(Markdown(\"#### Plain model - without debiasing - dataset metrics\"))\n",
    "# print(\"Train set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_dataset_nodebiasing_train.mean_difference())\n",
    "# print(\"Test set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_dataset_nodebiasing_test.mean_difference())\n",
    "\n",
    "# Metrics for the dataset from model with debiasing\n",
    "display(Markdown(\"#### Model - with debiasing - dataset metrics\"))\n",
    "metric_dataset_debiasing_train = BinaryLabelDatasetMetric(dataset_debiasing_train, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "\n",
    "print(\"Train set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_dataset_debiasing_train.mean_difference())\n",
    "\n",
    "metric_dataset_debiasing_test = BinaryLabelDatasetMetric(dataset_debiasing_test, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "\n",
    "print(\"Test set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_dataset_debiasing_test.mean_difference())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}